%!TEX root = ../thesis.tex

\chapter{Discussion}
This chapter discusses the results obtained from this study, as well as a subjective conclusion of the discussed frameworks.

\section{Performance} \label{sec:DiscussionPerformance}
Although not in focus in this study, the performance is always an interesting factor when comparing frameworks, and although a lot of comparison studies have been made between CUDA and OpenCL, DirectCompute is usually left out of the picture. The outcome from these kind of studies tend to show that CUDA usually outperforms the other frameworks, although studies show that the performance can be very similar under a fair comparison as previously discussed in section \ref{subsec:frameworkComparison}.

Figure \ref{fig:GraphTotExecTime} shows the total execution time for the various frameworks, as well as the execution time for the sequential implementation. As expected, the parallel GPGPU implementations outperform the sequential implementation considerably. For small N-body systems (~$0.2*10^4$), the performance of the sequential compared to the parallel implementations are very similar but as the size of the problem grows the parallel implementations clearly outperforms the sequential implementation and for very large problem sizes (~$2.0*10^4$) the speedup between the sequential and the fastest parallel implementation OpenCL is ~$4$. 

Surprisingly, CUDA which is usually the fastest framework as discussed in section \ref{subsec:frameworkComparison}, is the slowest of the evaluated frameworks in this implementation which is more clearly presented in figure \ref{fig:GPUStepExecTime}. There could be many reasons why this is the case. Due to the nature of how the tree is structured and traversed, it is difficult to achieve coalesced memory accesses in the kernel which may prove that CUDA handles non coalesced memory accesses more poorly than the other frameworks. 

Furthermore, we can see that the although more unfamiliar framework DirectCompute is a strong competitor to both CUDA and OpenCL. For small problem sizes DirectCompute outperforms both CUDA and OpenCL, and has a similar performance compared to OpenCL for bigger problem sizes. 

Visualized in figure \ref{fig:CUDAExecTime} - \ref{fig:SeqExecTime}, we can see that although the parts of the algorithm that is performed on the device, i.e. the force calculation and position update scales better than the tree construction and the tree flattening which are the main bottlenecks of the performance. Although more complex, both of these steps can be performed in parallel on the device as described by M. Burtscher and K. Pingali \cite{burtscher2011efficient}. By moving these steps, along with the COM calculation, the overhead spawned by coping the data back and forth between the host and the device every timestep would be eliminated and may further increase the performance of the application. 

Figure \ref{fig:SeqExecTime} which shows the execution time of the sequential implementation, shows that the force calculation and the position update scales very poorly when run sequentially and is thus the parts of the algorithm that are the most suited for beeing parallelized. 

Out of the evaluated frameworks, CUDA is the only one able to copy class objects in a buffer to the kernel. This eliminates the need to convert the class object based octree into a data structure based octree. Although this data conversion is a fast operation which is included in the Calc. Tree COM step, it is still interesting to compare how the use of class objects buffers affect the performance. Figure \ref{fig:GraphCUDAStructVsClass} shows the execution time for CUDA when using structured buffers and class object buffers and since the execution time of these are very similar we can conclude that the usage of class object buffers in this case does not hurt the performance, while at the same time keeps the code less complex. 


Another feature only available in CUDA is the ability to use recursion in kernels. This can be utilized in the force calculation when traversing the tree as discussed in section \ref{sec:CUDAImplementation}. The execution time when using a recursive tree traversal was measured and compared to the iterative tree traversal used in the other implementations. The result of this comparison is presented in figure \ref{fig:CUDARecursivePerformance}. As expected, the recursive implementation is more time consuming. The most likely cause of this is due to the amount of overhead spawned when allocating stack memory.




\section{Features} \label{sec:DiscussionFeatures}
To be able to compare the features of the frameworks, the documentation of the frameworks was studied and summarized in table \ref{tab:FrameworkFeatures}. The table summarizes some of the most important features of the respective framework and although a lot of the features are shared, CUDA has the advantage feature wise. Features such as kernel classes, recursion, dynamic parallelism and class object buffers are only supported in CUDA. These features are however very handy and can simplify and abstract the code significantly.

Out of the features listed in table \ref{tab:FrameworkFeatures} only available in CUDA, kernel classes, kernel recursion and class object buffers was used. This made the readability and understandably of the code much better since it abstracted away a lot of the complexity. 

Since neither OpenCL 1.2 or DirectCompute supports classes or C++ like data structures inside the kernel/CS code, the queue had to be inlined as discussed in section  \ref{sec:OpenCLImplementation} and \ref{sec:DirectComputeImplementation} which decreased the readability of the application. For applications using larger more complex kernels the ability to be able to write object oriented code is an important feature, and if the entirety of this implementation would be performed on the device this feature would be of most importance, making this a strong feature for CUDA to have. 

Recursion was utilized when calculating the forces which also simplified the complexity and readability. Although recursion simplified the tree traversal, it came at the cost of performance as discussed in section \ref{sec:DiscussionPerformance}. Since the overhead spawned by allocating stack memory for the recursive calls is difficult to avoid, recursion often hurt the performance and whether it should be used is a question about the performance-readability trade-off. 

Although not used in this implementation, dynamic parallelism is a feature only available in CUDA (available in OpenCL 2.0, see \ref{sec:ResultFeatures}) which could be an alternative to recursive function calls. Since it was not used in this implementation it is difficult to tell how dynamic parallelism would affect the performance, although in a paper by J. DiMarco and M. Taufer where dynamic parallelism was used on different clustering algorithms, speedups up to 3.03 times was observed and dynamic parallelism scaled better for larger problem sizes than when not using dynamic parallelism \cite{dimarco2013performance}. Another paper by J. Wang and S. Yalamanchili showed that using dynamic parallelism could achieve 1.13x-2.73x potential speedup but the overhead caused by the kernel launching could negate the performance benefit. \cite{wang2014characterization}





\section{Portability}





\section{Complexity}

As discussed in section \ref{sec:ResultComplexity}, to be able to get a good understanding of the complexity of the various frameworks, a vector addition application was implemented in the various frameworks. The code for these are given in Appendix \ref{appendix:CUDAVecAdd}, \ref{appendix:OpenCLVecAdd} and \ref{appendix:DirectComputeVecAdd}. The result of the measurements are summarized in table \ref{tab:VecAddMetrics}.

Out of the evaluated frameworks, CUDA is the least complex with a max complexity of 4, and an average complexity of 0.97 which is also reflected in the attached code example in appendix \ref{appendix:CUDAVecAdd}. With only 45 lines of code, a working CUDA vector addition can be implemented. Furthermore, since CUDA uses its own NVCC compiler, the device code can be included in the same file as the CUDA host code, further increasing the readability of the implementation. The actual kernel launch is very similar to a normal function call, making it more intuitive.

With a max complexity of 4, and a average complexity of 2.5, OpenCL is the second least complex framework. Since the developer has to specify the target device, the additional step of finding a compatible device and creating a context has to be implemented, and thus the size of the OpenCL implementation grows to 68 lines. Although the feature of selecting a device is possible in CUDA as well, it is not necessary since the framework per default selects the default GPU. The process of copy data to the device is done by using buffer objects and filling the buffers with the relevant data, whilst in CUDA the data can be copied directly to the device. The usage of buffers adds another step to the process, further decreasing the intuitiveness of the application. The actual copying of the data and the kernel launch is done by using a command queue. For a developer without a good knowledge in GPGPU, this may further increase the intuitiveness of the application. 

The most complex of the vector addition examples is the DirectCompute implementation with a max complexity of 9 and an average complexity of 4.41. This is also reflected in the size of the application with 241 lines, compared to only 45 in CUDA and 68 in OpenCL. One of the reason why this is the case is because different Direct3D 10 and 11 uses different functions for doing the same thing, why these cases has to be tested. Similar to OpenCL, DirectCompute also requires the developer to specify the target compute device and create the context, making the implementation a bit more cumbersome than CUDA. Also similar to OpenCL, DirectCompute handles the data transfer between the host and the device by using buffers, although a second step is required before the buffer can be copied to the device. The buffers have to be converted into access view, readable by the compute shader, making the intuitiveness worse. Although the process of dispatching the CS is trivial, the process of retrieving the data to the host is a bit more complex and requires the usage of a debug buffer, as well as mapping the resulting data to a mapped sub-resource, making the implementation more complex. 


%SUBJECTIVE DISCUSSION
From a subjective point of view, CUDA is the most intuitive of the various frameworks. It's API is well developed and forces the developer to write well structured and readable code. Out of the evaluated frameworks CUDA is the one that mostly resembles sequential programming which most developers are used to. Although OpenCL is similar to CUDA in many ways, it more resembles graphical programming with the use of buffers. DirectCompute is the most complex of the frameworks, but DirectX or similar graphics experience greatly facilitates the implementation and understanding. Furthermore, as illustrated in figure \ref{fig:GoogleTrendsPopularity}, CUDA and OpenCL is the most popular GPGPU frameworks and is well documented by the GPGPU community, making it easy to find sample applications and getting help from the community which may also be an important factor. Since DirectCompute is relatively unknown, this is more difficult and sample applications using DirectCompute is difficult to find. The only examples found in this work was examples implemented by Microsoft. 





\section{Encountered problems}

This section describes problems that arose during the implementation, and how they were solved.

\subsubsection{Copy the Barnes-Hut Octree to the device}
One of the encountered problems that arose during the implementation phase was how to copy the Octree, built on the host, to the device in order to calculate the net forces applied on the bodies. The problem is that all of the evaluated frameworks requires information about how large an element that is to be copied to the device is. The pointer based octree is a very dynamic data structure and since it is rebuilt each step, might vary greatly in size. A second problem with the pointer based octree is that the pointers point to memory location in the RAM, which thus has to be updated after the octree has been copied to the device.

One way in which this could be solved would be to traverse the octree using either a recursive or iterative tree traversal algorithm, calculating the size as it traverses the octree. However, this solution would still make the octree reliant on a pointer based structure which would result in a lot of pointer chasing on the device.

The second and more common solution is to flatten the tree into a simpler data structure. Similarly the tree has to be traversed and each node of the octree has to be copied to the simpler data structure such as an array. 
This was the choosen solution, which also eliminated the need for pointers since each node in the tree can be represented as an index in the array.

\subsubsection{Iteratively traverse the octree}
Since CUDA is the only framework which allows recursion in the kernel code, a way to traverse the octree in an iterative manner had to be explored. This is however a common problem which is usually solved by using either a stack or a queue. 

Since there exists no queue or stack functionality in any of the frameworks kernel code, a simple queue was implemented. In CUDA, which supports C++ like features, the implementation was made as a structure with member functions and default variable values. Since neither DirectCompute or OpenCL 1.2 which was used in this implementation supports these C++ features, the queue code had to be inlined, resulting in a more complex code structure.

\subsubsection{Copying class objects to the kernel}
Out of the frameworks that was evaluated in this study, CUDA is the only framework that supports class objects in the buffer which is passed to the kernel. Both DirectCompute and OpenCL (1.2) does however support buffers containing data structures. The solution was thus to convert the class object based octree into data structure, which could be done whilst flattening the tree to avoid unnecessary performance degradation. 
 